{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing \n",
    "- remove stopwords from NLTK corpus and some negation words \n",
    "- lowercase all words\n",
    "- used regular expression to remove all non-alphabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "modStopwords = [word for word in stopwords if word not in ['not', 'no', 'can', 'don', 't']]\n",
    "\n",
    "def preProcessdata(data):\n",
    "    wordList = re.split('\\s+', data.lower())\n",
    "    punctuation = re.compile(r'[-.?!/\\%@,\":;()|0-9]')\n",
    "    wordList = [punctuation.sub(\"\", word) for word in wordList]\n",
    "    finalWordList = []\n",
    "    for word in wordList:\n",
    "        if word not in modStopwords:\n",
    "            finalWordList.append(word)\n",
    "    res = \" \".join(finalWordList)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateWords(data):\n",
    "    wordDict = []\n",
    "    for (words, sentiment) in data:\n",
    "        possible_words = [x for x in words if len(x) >= 3]\n",
    "        wordDict.extend(possible_words)\n",
    "    return wordDict\n",
    "\n",
    "\n",
    "def generateWordsnormal(data):\n",
    "    wordDict = []\n",
    "    for (words, sentiment) in data:\n",
    "        wordDict.extend(words)\n",
    "    return wordDict\n",
    "\n",
    "\n",
    "def generateallWords(data):\n",
    "    wordDict = []\n",
    "    for id, words in data:\n",
    "        wordDict.extend(words)\n",
    "    return wordDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Featuresets\n",
    "The main idea of this section is to use unigram and bigram feature to train and test a Naive Bayes Classifier \n",
    "- Use bag-of-word method (unigram feature) to find the top 200 most frequent words, and define them as wordFeatures \n",
    "- Also define a bigram feature. I will use BoW model as the baseline for model accuracy, and see what methods can help improve model accuracy comparing BoW model\n",
    "- For unigram feature, label each keywrod in wrodFeatures as 'V_keyword' also labeled bigram features as V_{}, B_{} _{}.\n",
    "- In addition to BOW and bigram featureset, combine two featureset  'combinedfeatureset' as a new featureset to test out if can improve model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagofWords(wordList):\n",
    "    wordlist = nltk.FreqDist(wordList)\n",
    "    wordFeatures = [w for (w, c) in wordlist.most_common(200)]\n",
    "    return wordFeatures\n",
    "\n",
    "\n",
    "def unigram_features(data, wordFeatures):\n",
    "    documentWords = set(data)\n",
    "    features = {}\n",
    "    for word in wordFeatures:\n",
    "        features['V_{}'.format(word)] = (word in documentWords)\n",
    "    return features\n",
    "\n",
    "def bigram_finder(wordList):\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(wordList, window_size=3)\n",
    "    bigram_features = finder.nbest(bigram_measures.chi_sq, 3000)\n",
    "    return bigram_features[:500]\n",
    "\n",
    "def bigram_features(data, wordFeatures, bigramFeatures):\n",
    "    document_words = set(data)\n",
    "    document_bigrams = nltk.bigrams(data)\n",
    "    features = {}\n",
    "    for word in wordFeatures:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    for bigram in bigramFeatures:\n",
    "        features['B_{}{})'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)\n",
    "    return features\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateCSV(featuresets, filePath):\n",
    "    filepath='/Users/jenniferchen/Desktop'\n",
    "    f = open(filePath, 'w')\n",
    "    featurenames = featuresets[0][0].keys()\n",
    "    featurenameline = ''\n",
    "    for featurename in featurenames:\n",
    "        featurename = featurename.replace(',', 'CM')\n",
    "        featurename = featurename.replace(\"'\", \"DQ\")\n",
    "        featurename = featurename.replace('\"', 'QU')\n",
    "        featurenameline += featurename + ','\n",
    "    featurenameline += 'class'\n",
    "    f.write(featurenameline)\n",
    "    f.write('\\n')\n",
    "    for featureset in featuresets:\n",
    "        featureline = ''\n",
    "        for key in featurenames:\n",
    "            featureline += str(featureset[0][key]) + ','\n",
    "        if featureset[1] == 0:\n",
    "            featureline += str(\"neg\")\n",
    "        elif featureset[1] == 1:\n",
    "            featureline += str(\"sneg\")\n",
    "        elif featureset[1] == 2:\n",
    "            featureline += str(\"neu\")\n",
    "        elif featureset[1] == 3:\n",
    "            featureline += str(\"spos\")\n",
    "        elif featureset[1] == 4:\n",
    "            featureline += str(\"pos\")\n",
    "        f.write(featureline)\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_document_features(document, word_features, bigram_features, SL):\n",
    "    document_words = set(document)\n",
    "    document_bigrams = nltk.bigrams(document)\n",
    "    features = {}\n",
    "    for word in document_words:\n",
    "        posword = 0\n",
    "        neutword = 0\n",
    "        negword = 0\n",
    "        for word in document_words:\n",
    "            if word in SL[0]:\n",
    "                posword += 1\n",
    "            if word in SL[1]:\n",
    "                neutword += 1\n",
    "            if word in SL[2]:\n",
    "                negword += 1\n",
    "            features['positivecount'] = posword\n",
    "            features['neutralcount'] = neutword\n",
    "            features['negativecount'] = negword\n",
    "        for word in word_features:\n",
    "            features['V_{}'.format(word)] = False\n",
    "            features['V_NOT{}'.format(word)] = False\n",
    "        for bigram in bigram_features:\n",
    "            features['B_{}_{}'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use Naive Bayes Classifier to train on the featuresets defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAccuracy(featuresets):\n",
    "    training_size = int(0.1 * len(featuresets))\n",
    "    test_set = featuresets[:training_size]\n",
    "    training_set = featuresets[training_size:]\n",
    "    classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "    print(\"Accuracy : \")\n",
    "    print(nltk.classify.accuracy(classifier, test_set))\n",
    "    generateMatrix(classifier, test_set)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subjectivity Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readSubjectivity(filePath):\n",
    "    flexicon = open(filePath, 'r')\n",
    "    slex = {}\n",
    "    for line in flexicon:\n",
    "        fields = line.split()\n",
    "        strength = fields[0].split(\"=\")[1]\n",
    "        word = fields[2].split(\"=\")[1]\n",
    "        posTag = fields[3].split(\"=\")[1]\n",
    "        stemmed = fields[4].split(\"=\")[1]\n",
    "        polarity = fields[5].split(\"=\")[1]\n",
    "        if (stemmed == 'y'):\n",
    "            isStemmed = True\n",
    "        else:\n",
    "            isStemmed = False\n",
    "        slex[word] = [strength, posTag, isStemmed, polarity]\n",
    "    return slex\n",
    "\n",
    "\n",
    "SLpath = \"/Users/jenniferchen/Desktop/subjclueslen1-HLTEMNLP05.tff\"\n",
    "SL = readSubjectivity(SLpath)\n",
    "\n",
    "\n",
    "def SL_features(document, word_features, SL):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    weakPos = 0\n",
    "    strongPos = 0\n",
    "    weakNeg = 0\n",
    "    strongNeg = 0\n",
    "    for word in document_words:\n",
    "        if word in SL:\n",
    "            strength, posTag, isStemmed, polarity = SL[word]\n",
    "            if strength == 'weaksubj' and polarity == 'positive':\n",
    "                weakPos += 1\n",
    "            if strength == 'strongsubj' and polarity == 'positive':\n",
    "                strongPos += 1\n",
    "            if strength == 'weaksubj' and polarity == 'negative':\n",
    "                weakNeg += 1\n",
    "            if strength == 'strongsubj' and polarity == 'negative':\n",
    "                strongNeg += 1\n",
    "            features['positivecount'] = weakPos + (2 * strongPos)\n",
    "            features['negativecount'] = weakNeg + (2 * strongNeg)\n",
    "    if 'positivecount' not in features:\n",
    "        features['positivecount'] = 0\n",
    "    if 'negativecount' not in features:\n",
    "        features['negativecount'] = 0\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processkaggle(dirPath, limitStr):\n",
    "    limit = 10000\n",
    "    #os.chdir(dirPath)\n",
    "    f = open('./train.tsv', 'r')\n",
    "    sentenceData = []\n",
    "    for line in f:\n",
    "        if (not line.startswith('Phrase')):\n",
    "            line = line.strip()\n",
    "            sentenceData.append(line.split('\\t')[2:4])\n",
    "    random.shuffle(sentenceData)\n",
    "    sentenceList = sentenceData[:limit]\n",
    "    print('Read', len(sentenceData), 'phrases, using', len(sentenceList), 'random phrases')\n",
    "    phrasedocs = []\n",
    "    phrasedocs_without = []\n",
    "    for phrase in sentenceList:\n",
    "        tokens = nltk.word_tokenize(phrase[0])\n",
    "        phrasedocs_without.append((tokens, int(phrase[1])))\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        phrase[0] = preProcessdata(phrase[0])\n",
    "        tokens = tokenizer.tokenize(phrase[0])\n",
    "        phrasedocs.append((tokens, int(phrase[1])))\n",
    "    normaltokens = generateWordsnormal(phrasedocs_without)\n",
    "    preprocessedTokens = generateWords(phrasedocs)\n",
    "    word_features = bagofWords(normaltokens)\n",
    "    featuresets_without_preprocessing = [(unigram_features(d, word_features), s) for (d, s) in phrasedocs_without]\n",
    "    print(\" \")\n",
    "    print(\"Accuracy without pre-processing unigram features : \")\n",
    "    generateAccuracy(featuresets_without_preprocessing)\n",
    "    word_features = bagofWords(preprocessedTokens)\n",
    "    featuresets = [(unigram_features(d, word_features), s) for (d, s) in phrasedocs]\n",
    "    print(\" \")\n",
    "    print(\"Accuracy with pre-processed unigram features : \")\n",
    "    generateAccuracy(featuresets)\n",
    "    SL_featuresets = [(SL_features(d, word_features, SL), c) for (d, c) in phrasedocs]\n",
    "    \n",
    "    print(\"Accuracy with SL_featuresets : \")\n",
    "    generateAccuracy(SL_featuresets)\n",
    "    bigram_feature = bigram_finder(preprocessedTokens)\n",
    "    bigram_featuresets = [(bigram_features(d, word_features, bigram_feature), c) for (d, c) in phrasedocs]\n",
    "    print(\" \")\n",
    "    print(\"Accuracy with bigram featuresets : \")\n",
    "    generateAccuracy(bigram_featuresets)\n",
    "    \n",
    "    '''\n",
    "    features_combined = [(combined_document_features(d, word_features, SL_featuresets, bigram_featuresets), c) for\n",
    "                         (d, c) in phrasedocs]\n",
    "    print(\"Accuracy with combined featuresets : \")\n",
    "    generateAccuracy(features_combined)\n",
    "    '''\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateMatrix(classifier_type, test_set):\n",
    "    goldlist = []\n",
    "    predictlist = []\n",
    "    for (features, label) in test_set:\n",
    "        goldlist.append(label)\n",
    "        predictlist.append(classifier_type.classify(features))\n",
    "    print(\"Confusion matrix\")\n",
    "    cm = ConfusionMatrix(goldlist, predictlist)\n",
    "    print(cm)\n",
    "    print(\"Confusion matrix in percentage\")\n",
    "    print(cm.pretty_format(sort_by_count=True,show_percents=True,truncate=9))\n",
    "    print('Model Evaluation')\n",
    "    eval_measures(goldlist,predictlist)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_measures(gold, predicted):\n",
    "    # get a list of labels\n",
    "    labels = list(set(gold))\n",
    "    # these lists have values for each label \n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    F1_list = []\n",
    "    for lab in labels:\n",
    "        # for each label, compare gold and predicted lists and compute values\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(gold):\n",
    "            if val == lab and predicted[i] == lab:  TP += 1\n",
    "            if val == lab and predicted[i] != lab:  FN += 1\n",
    "            if val != lab and predicted[i] == lab:  FP += 1\n",
    "            if val != lab and predicted[i] != lab:  TN += 1\n",
    "        # use these to compute recall, precision, F1\n",
    "        recall = TP / (TP + FP)\n",
    "        precision = TP / (TP + FN)\n",
    "        recall_list.append(recall)\n",
    "        precision_list.append(precision)\n",
    "        F1_list.append( 2 * (recall * precision) / (recall + precision))\n",
    "\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    print('\\tPrecision\\tRecall\\t\\tF1')\n",
    "    # print measures for each label\n",
    "    for i, lab in enumerate(labels):\n",
    "        print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "          \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_submission(setFeatures, test_featuresets, fileName):\n",
    "    print(\"Training and testing a classifier \")\n",
    "    test_set = test_featuresets\n",
    "    training_set = setFeatures\n",
    "    classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "    fw = open(fileName, \"w\")\n",
    "    fw.write(\"PhraseId\" + ',' + \"Sentiment\" + '\\n')\n",
    "    for test, id in test_featuresets:\n",
    "        fw.write(str(id) + ',' + str(classifier.classify(test)) + '\\n')\n",
    "    fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_document_features(document, word_features, bigram_features, SL):\n",
    "    document_words = set(document)\n",
    "    document_bigrams = nltk.bigrams(document)\n",
    "    features = {}\n",
    "    for word in document_words:\n",
    "        posword = 0\n",
    "        neutword = 0\n",
    "        negword = 0\n",
    "        for word in document_words:\n",
    "            if word in SL[0]:\n",
    "                posword += 1\n",
    "            if word in SL[1]:\n",
    "                neutword += 1\n",
    "            if word in SL[2]:\n",
    "                negword += 1\n",
    "            features['positivecount'] = posword\n",
    "            features['neutralcount'] = neutword\n",
    "            features['negativecount'] = negword\n",
    "        for word in word_features:\n",
    "            features['V_{}'.format(word)] = False\n",
    "            features['V_NOT{}'.format(word)] = False\n",
    "        for bigram in bigram_features:\n",
    "            features['B_{}_{}'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 156060 phrases, using 10000 random phrases\n",
      " \n",
      "Accuracy without pre-processing unigram features : \n",
      "Accuracy : \n",
      "0.502\n",
      "Confusion matrix\n",
      "  |   0   1   2   3   4 |\n",
      "--+---------------------+\n",
      "0 | <13>  7  18   6   . |\n",
      "1 |  20 <27>116  15   6 |\n",
      "2 |  12  33<410> 36   8 |\n",
      "3 |  15   8 137 <45> 10 |\n",
      "4 |   7   1  34   9  <7>|\n",
      "--+---------------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Confusion matrix in percentage\n",
      "  |      2      3      1      4      0 |\n",
      "--+------------------------------------+\n",
      "2 | <41.0%>  3.6%   3.3%   0.8%   1.2% |\n",
      "3 |  13.7%  <4.5%>  0.8%   1.0%   1.5% |\n",
      "1 |  11.6%   1.5%  <2.7%>  0.6%   2.0% |\n",
      "4 |   3.4%   0.9%   0.1%  <0.7%>  0.7% |\n",
      "0 |   1.8%   0.6%   0.7%      .  <1.3%>|\n",
      "--+------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Model Evaluation\n",
      "\tPrecision\tRecall\t\tF1\n",
      "0 \t      0.295      0.194      0.234\n",
      "1 \t      0.147      0.355      0.208\n",
      "2 \t      0.822      0.573      0.675\n",
      "3 \t      0.209      0.405      0.276\n",
      "4 \t      0.121      0.226      0.157\n",
      "\n",
      "\n",
      " \n",
      "Accuracy with pre-processed unigram features : \n",
      "Accuracy : \n",
      "0.5\n",
      "Confusion matrix\n",
      "  |   0   1   2   3   4 |\n",
      "--+---------------------+\n",
      "0 |  <1> 10  30   3   . |\n",
      "1 |   6 <23>139  13   3 |\n",
      "2 |   4  23<435> 35   2 |\n",
      "3 |   3  10 164 <36>  2 |\n",
      "4 |   2   1  34  16  <5>|\n",
      "--+---------------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Confusion matrix in percentage\n",
      "  |      2      3      1      4      0 |\n",
      "--+------------------------------------+\n",
      "2 | <43.5%>  3.5%   2.3%   0.2%   0.4% |\n",
      "3 |  16.4%  <3.6%>  1.0%   0.2%   0.3% |\n",
      "1 |  13.9%   1.3%  <2.3%>  0.3%   0.6% |\n",
      "4 |   3.4%   1.6%   0.1%  <0.5%>  0.2% |\n",
      "0 |   3.0%   0.3%   1.0%      .  <0.1%>|\n",
      "--+------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Model Evaluation\n",
      "\tPrecision\tRecall\t\tF1\n",
      "0 \t      0.023      0.062      0.033\n",
      "1 \t      0.125      0.343      0.183\n",
      "2 \t      0.872      0.542      0.669\n",
      "3 \t      0.167      0.350      0.226\n",
      "4 \t      0.086      0.417      0.143\n",
      "\n",
      "\n",
      "Accuracy with SL_featuresets : \n",
      "Accuracy : \n",
      "0.527\n",
      "Confusion matrix\n",
      "  |   0   1   2   3   4 |\n",
      "--+---------------------+\n",
      "0 |  <2> 16  21   4   1 |\n",
      "1 |   6 <28>124  23   3 |\n",
      "2 |   5  28<409> 48   9 |\n",
      "3 |   3  10 120 <74>  8 |\n",
      "4 |   2   3  14  25 <14>|\n",
      "--+---------------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Confusion matrix in percentage\n",
      "  |      2      3      1      4      0 |\n",
      "--+------------------------------------+\n",
      "2 | <40.9%>  4.8%   2.8%   0.9%   0.5% |\n",
      "3 |  12.0%  <7.4%>  1.0%   0.8%   0.3% |\n",
      "1 |  12.4%   2.3%  <2.8%>  0.3%   0.6% |\n",
      "4 |   1.4%   2.5%   0.3%  <1.4%>  0.2% |\n",
      "0 |   2.1%   0.4%   1.6%   0.1%  <0.2%>|\n",
      "--+------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Model Evaluation\n",
      "\tPrecision\tRecall\t\tF1\n",
      "0 \t      0.045      0.111      0.065\n",
      "1 \t      0.152      0.329      0.208\n",
      "2 \t      0.820      0.594      0.689\n",
      "3 \t      0.344      0.425      0.380\n",
      "4 \t      0.241      0.400      0.301\n",
      "\n",
      "\n",
      " \n",
      "Accuracy with bigram featuresets : \n",
      "Accuracy : \n",
      "0.5\n",
      "Confusion matrix\n",
      "  |   0   1   2   3   4 |\n",
      "--+---------------------+\n",
      "0 |  <1> 10  30   3   . |\n",
      "1 |   6 <23>139  13   3 |\n",
      "2 |   4  23<435> 35   2 |\n",
      "3 |   3  10 164 <36>  2 |\n",
      "4 |   2   1  34  16  <5>|\n",
      "--+---------------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Confusion matrix in percentage\n",
      "  |      2      3      1      4      0 |\n",
      "--+------------------------------------+\n",
      "2 | <43.5%>  3.5%   2.3%   0.2%   0.4% |\n",
      "3 |  16.4%  <3.6%>  1.0%   0.2%   0.3% |\n",
      "1 |  13.9%   1.3%  <2.3%>  0.3%   0.6% |\n",
      "4 |   3.4%   1.6%   0.1%  <0.5%>  0.2% |\n",
      "0 |   3.0%   0.3%   1.0%      .  <0.1%>|\n",
      "--+------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Model Evaluation\n",
      "\tPrecision\tRecall\t\tF1\n",
      "0 \t      0.023      0.062      0.033\n",
      "1 \t      0.125      0.343      0.183\n",
      "2 \t      0.872      0.542      0.669\n",
      "3 \t      0.167      0.350      0.226\n",
      "4 \t      0.086      0.417      0.143\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) != 3:\n",
    "        print('usage: classifyKaggle.py <corpus-dir> <limit>')\n",
    "        sys.exit(0)\n",
    "    processkaggle(sys.argv[1], sys.argv[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
